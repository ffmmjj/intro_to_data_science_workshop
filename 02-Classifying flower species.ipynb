{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification** problems are a broad category of machine learning problems that involve the prediction of values taken from a discrete, finite number of cases. \n",
    "\n",
    "In this example, we'll build a classifier to predict to which species a flower belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = # read the file 'datasets/iris.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some info about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique classes present in the dataset using the method unique() in the Class column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the describe() method to print summary statistics about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the classes to numeric values\n",
    "class_encodings = # create a dictionary mapping each class to a numeric value\n",
    "\n",
    "iris['Class'] = # Use the map() method to convert the class strings to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['Class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot for sepal length and sepal width\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sl = iris['Sepal_length']\n",
    "sw = iris['Sepal_width']\n",
    "\n",
    "# Create a scatterplot of these two properties using plt.scatter()\n",
    "# Assign different colors to each data point according to the class it belongs to\n",
    "\n",
    "# Specify labels for the X and Y axis\n",
    "\n",
    "# Show graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot for petal length and petal width\n",
    "pl = iris['Petal_length']\n",
    "pw = iris['Petal_width']\n",
    "\n",
    "# Create a scatterplot of these two properties using plt.scatter()\n",
    "# Assign different colors to each data point according to the class it belongs to\n",
    "\n",
    "# Specify labels for the X and Y axis\n",
    "\n",
    "# Show graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [scikit-learn's LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to build out classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = # Create a dataset with all the features by dropping the 'Class' column\n",
    "t = # Get the 'Class' column values\n",
    "RANDOM_STATE = 4321\n",
    "\n",
    "# Use sklean's train_test_plit() method to split our data into two sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, t, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the training set to build a LogisticRegression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = # Create a logistic regression model\n",
    "# Fit the data to the model using the fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LogisticRegression's score() method to assess the model accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting classification results\n",
    "\n",
    "Scores like the one calculated above are usually not what we want to assess. it will only return the mean error obtained between predictions and the actual classes in the training dataset. \n",
    "\n",
    "Consider what happens, for instance, when you're training a model to classify if someone has a disease or not and 99% of the people don't have that disease. What can go wrong if you use a score like the one above to evaluate your model? *Hint: What would be the score of a classifier that always returns zero(i.e. it always says that the person doesn't have the disease) in this case?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple score metrics are usually not recommended for classification problems. There are at least three different metrics that are commonly used depending on the context:\n",
    "* **Precision**: This is the number of true positives that the classifier got right - in the example of the disease classifier, this metric would say how many of the people who it said would have the disease _actually_ have that disease;\n",
    "* **Recall**: This is the number of true positives that are found by the classifier - in the same example, this metric would tell us how many of the people who actually have the disease were _found_ by the classifier;\n",
    "* **F1-Score**: This is a weighted sum of precision and recall - it's not easy to interpret its value intuitively, but the idea is that the f1-score represents a compromise between precision and recall;\n",
    "\n",
    "<img src='images/Precisionrecall.svg'></img>\n",
    "Source: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "Some other common evaluation methods for classification models include [ROC chart analysis](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and the related concept of [Area Under Curve (AUC)](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it).\n",
    "\n",
    "*What metric would you prioritise in the case of the disease classifier described before? What are the costs of false positives and false negatives in this case?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn provides a function called \"classification_report\" that summarizes the three metrics above\n",
    "# for a given classification model on a dataset.\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use this function to print a classification metrics report for the trained classifier.\n",
    "# See http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful technique to inspect the results given by a classification model is to take a look at its *confusion matrix*. This is an K x K matrix (where K is the number of distinct classes identified by the classifier) that gives us, in the position **(i, j)**, how many examples belonging to class **i** were classified as belonging to class **j**. \n",
    "\n",
    "That can give us insights on which classes may require more attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use scikit-learn's confusion_matrix to understand which classes were misclassified.\n",
    "# See http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the example above, what would you investigate? What classes is the classifier having difficulty to discriminate?*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
