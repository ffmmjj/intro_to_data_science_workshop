{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we didn't know how many different species there we in the Iris dataset. How can we approximately infer this information from the data?\n",
    "\n",
    "One possible solution would be to plot the data as a scatter plot and visually identify distinct groups. The Iris dataset, however, is comprised of four dimensions, so it can't be visualized except for pairs of features.\n",
    "\n",
    "In order to visualize the complete dataset as a 2D scatterplot, it's possible to use **dimensionality reduction** techniques to reduce the data to two dimensions without losing too much structural information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = # Read the file 'datasets/iris_without_classes.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first entries using the head() method to check that there is no Class information anymore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [scikit-learn's PCA](http://scikit-learn.org/stable/modules/decomposition.html#pca) algorithm to reduce the number of dimensions to two in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA's fit_transform() method to reduce the dataset size to two dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "RANDOM_STATE=1234\n",
    "pca_model = # Create a PCA object with two components\n",
    "iris_2d = # use fit_transform() to reduce the original dataset into two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot of the reduced dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a scatterplot of the two dimensions of the transformed data\n",
    "\n",
    "# Show the scatterplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many distinct groups can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding clusters using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented above can be framed as a **Clustering** problem. Clustering involves finding groups of examples that are like other examples in the same group but different from examples that belong to other groups.\n",
    "\n",
    "In this example, we'll use [scikit-learn's KMeans](http://scikit-learn.org/stable/modules/clustering.html#k-means) algorithm to find clusters in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of KMeans is that it receives the expected number of clusters as input, so you must either have some domain knowledge to guess a reasonable number of groups or attempt different values for the number of clusters and see which one works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two KMeans models: one with two clusters and another with three clusters\n",
    "# Store the labels predicted by the KMeans models using two and three clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model2 =  # Create a KMeans model expecting two clusters\n",
    "labels2 = # Predict the cluster label for each data point using predict()\n",
    "\n",
    "model3 =  # Create a KMeans model expecting three clusters\n",
    "labels3 = # Predict the cluster label for each data point using predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2-clusters assignments using the reduced dataset. Use different colors for each cluster\n",
    "\n",
    "# Show the scatterplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 3-clusters assignments using the reduced dataset. Use different colors for each cluster\n",
    "\n",
    "# Show the scatterplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some techniques like Silhouette Analysis to automatically infer the optimal number of clusters in a dataset. [This link](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) gives an example of how that could be done using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for dimensionaliy reduction, PCA is one of the most common techniques for a first attempt. Some common alternatives to KMeans and PCA are respectivelly [DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) and [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). For a nice interactive overview of t-SNE in action, see [this link](http://distill.pub/2016/misread-tsne/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
